{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import triton\n",
    "import triton.language as tl\n",
    "import torch\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from typing import Tuple, List\n",
    "import psutil\n",
    "import os\n",
    "\n",
    "@triton.jit\n",
    "def flash_attn_v2_fwd_kernel(\n",
    "    Q, K, V, O, LSE, # Add LSE output tensor\n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,  # Q strides: Batch, Head, SeqLen, HeadDim\n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,  # K strides: Batch, Head, SeqLen, HeadDim\n",
    "    stride_vz, stride_vh, stride_vn, stride_vk,  # V strides: Batch, Head, SeqLen, HeadDim\n",
    "    stride_oz, stride_oh, stride_om, stride_ok,  # O strides: Batch, Head, SeqLen, HeadDim\n",
    "    stride_lse_z, stride_lse_h, stride_lse_m,    # LSE strides: Batch, Head, SeqLen\n",
    "    Z, H, N_CTX, D_HEAD,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_K: tl.constexpr, # Note: BLOCK_SIZE_K is same as D_HEAD in this simplified version\n",
    "    IF_CAUSAL_MASK: tl.constexpr,\n",
    "):\n",
    "\n",
    "    # Loadin the indcies of batch and head and axis\n",
    "    start_m = tl.program_id(axis=0) # Block row index\n",
    "    batch_head_id = tl.program_id(axis=1) # Batch and Head index combined\n",
    "\n",
    "    # Loadin the indcies of batch and head\n",
    "    batch_id = batch_head_id // H\n",
    "    head_id = batch_head_id % H\n",
    "\n",
    "    # Row offsets for the Q block (BLOCK_SIZE_M rows)\n",
    "    offs_m = start_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    # Head dimension offsets (BLOCK_SIZE_K columns, which is D_HEAD)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K) # BLOCK_SIZE_K == D_HEAD\n",
    "\n",
    "    # Q pointers: Shape (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "    q_ptrs = (Q + batch_id * stride_qz + head_id * stride_qh +\n",
    "              offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n",
    "\n",
    "    # K/V base pointers for the current batch and head\n",
    "    # We will add sequence dimension offsets in the loop\n",
    "    k_base_ptr = K + batch_id * stride_kz + head_id * stride_kh\n",
    "    v_base_ptr = V + batch_id * stride_vz + head_id * stride_vh\n",
    "\n",
    "    # --- Initialize ---\n",
    "    # Accumulator for the output O = Softmax(QK^T)V\n",
    "    # Needs to be float32 for precision\n",
    "    acc = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_K), dtype=tl.float32)\n",
    "    # Statistics for online softmax:\n",
    "    # Max value encountered so far per row: m_i\n",
    "    m_i = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32) - float('inf') # Initialize max to -inf\n",
    "    # Sum of exp(x - max) encountered so far per row: l_i\n",
    "    l_i = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32) # Initialize sum to 0\n",
    "\n",
    "    # Scale factor for QK^T (usually 1 / sqrt(D_HEAD))\n",
    "    qk_scale = tl.rsqrt(D_HEAD.to(tl.float32)) # Use 1.0/sqrt for clarity\n",
    "\n",
    "    # --- Load Q Tile ---\n",
    "    # Load Q for the current block row (offs_m)\n",
    "    # Boundary check for Q rows (sequence length N_CTX)\n",
    "    q_mask = offs_m[:, None] < N_CTX\n",
    "    # Boundary check for Q/K/V columns (head dimension D_HEAD) - implicit via BLOCK_SIZE_K == D_HEAD\n",
    "    # If BLOCK_SIZE_K could be < D_HEAD, we'd need a mask: (offs_k[None, :] < D_HEAD)\n",
    "    q = tl.load(q_ptrs, mask=q_mask, other=0.0) # Shape: (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "\n",
    "    # --- Determine Loop Bounds for K/V Blocks ---\n",
    "    # For causal mask, we only need K/V blocks up to the current Q block's end\n",
    "    # Otherwise, we need all K/V blocks\n",
    "    end_n = N_CTX\n",
    "    if IF_CAUSAL_MASK:\n",
    "      # K/V blocks should end where Q block ends\n",
    "      # For token i (row in Q), we only attend to tokens j <= i (columns in K/V)\n",
    "      end_n = (start_m + 1) * BLOCK_SIZE_M\n",
    "\n",
    "    # --- Loop over K/V Blocks (Columns of QK^T matrix) ---\n",
    "    for start_n in range(0, end_n, BLOCK_SIZE_N):\n",
    "        # --- Load K and V Tiles ---\n",
    "        # Column offsets for the current K/V block (BLOCK_SIZE_N columns)\n",
    "        offs_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "        # K pointers: Shape (BLOCK_SIZE_K, BLOCK_SIZE_N) because K is transposed for matmul\n",
    "        k_ptrs = (k_base_ptr +\n",
    "                  offs_n[:,None] * stride_kn + offs_k[None,:] * stride_kk)\n",
    "        # V pointers: Shape (BLOCK_SIZE_N, BLOCK_SIZE_K)\n",
    "        v_ptrs = (v_base_ptr +\n",
    "                  offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk) # Corrected V pointer\n",
    "\n",
    "        # Boundary check masks for K and V tiles\n",
    "        # K mask depends on N_CTX for columns (offs_n) and D_HEAD for rows (offs_k)\n",
    "        k_mask = (offs_n[None, :] < N_CTX)# & (offs_k[:, None] < D_HEAD) # D_HEAD check implicit\n",
    "        # V mask depends on N_CTX for rows (offs_n) and D_HEAD for columns (offs_k)\n",
    "        v_mask = (offs_n[:, None] < N_CTX)# & (offs_k[None, :] < D_HEAD) # D_HEAD check implicit\n",
    "\n",
    "        # Load K tile (transposed layout for dot product)\n",
    "        k = tl.load(k_ptrs, mask=k_mask, other=0.0) # Shape: (BLOCK_SIZE_K, BLOCK_SIZE_N)\n",
    "        # Load V tile\n",
    "        v = tl.load(v_ptrs, mask=v_mask, other=0.0) # Shape: (BLOCK_SIZE_N, BLOCK_SIZE_K)\n",
    "\n",
    "        # --- Compute QK^T Score Block ---\n",
    "        # q shape: (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "        # k shape: (BLOCK_SIZE_K, BLOCK_SIZE_N)\n",
    "        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)\n",
    "        qk += tl.dot(q, k,trans_b=True) # Shape: (BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
    "        qk *= qk_scale\n",
    "\n",
    "        # --- Apply Causal Mask (if enabled) ---\n",
    "        if IF_CAUSAL_MASK:\n",
    "            # Create mask where q_row_index >= k_col_index\n",
    "            # offs_m are row indices, offs_n are column indices\n",
    "            causal_mask = offs_m[:, None] >= offs_n[None, :]\n",
    "            # Apply mask: Set scores for future tokens to negative infinity\n",
    "            qk = tl.where(causal_mask, qk, -float('inf'))\n",
    "\n",
    "        # --- Online Softmax Calculation ---\n",
    "        # 1. Find the new maximum across the block's scores and the old maximum\n",
    "        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1)) # Shape: (BLOCK_SIZE_M,)\n",
    "\n",
    "        # 2. Calculate probabilities P_ij = exp(qk_ij - m_i_new)\n",
    "        # Subtracting the new max prevents overflow\n",
    "        p_ij = tl.exp2(qk - m_i_new[:, None]) # Shape: (BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
    "\n",
    "        # 3. Calculate scaling factor for previous accumulator and l_i\n",
    "        # scale = exp(m_i_old - m_i_new)\n",
    "        scale = tl.exp2(m_i - m_i_new) # Shape: (BLOCK_SIZE_M,)\n",
    "\n",
    "        # 4. Rescale previous accumulator: acc = acc * scale\n",
    "        # Convert acc to float32 before scaling if it's not already\n",
    "        acc = acc * scale[:, None] # Shape: (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "\n",
    "        # 5. Update accumulator: acc = acc + P_ij @ V_j\n",
    "        # p_ij shape: (BLOCK_SIZE_M, BLOCK_SIZE_N)\n",
    "        # v shape: (BLOCK_SIZE_N, BLOCK_SIZE_K)\n",
    "        # Ensure matching types for dot product\n",
    "        acc += tl.dot(p_ij.to(v.dtype), v) # Corrected dot product\n",
    "\n",
    "        # 6. Update the sum denominator: l_i = l_i * scale + sum(P_ij, axis=1)\n",
    "        # Calculate block sum: l_i_current = sum(P_ij, axis=1)\n",
    "        l_i_current = tl.sum(p_ij, axis=1) # Shape: (BLOCK_SIZE_M,)\n",
    "        # Rescale previous l_i and add current block's contribution\n",
    "        l_i = l_i * scale + l_i_current # Shape: (BLOCK_SIZE_M,)\n",
    "\n",
    "        # 7. Update running max for next iteration: m_i = m_i_new\n",
    "        m_i = m_i_new\n",
    "        # --- End of Loop Iteration ---\n",
    "\n",
    "    # --- Post-Loop Calculation and Storage ---\n",
    "\n",
    "\n",
    "    # LSE = m_i + log(l_i)\n",
    "    log_l_i = tl.log(l_i)\n",
    "    lse_final = m_i + log_l_i # Shape: (BLOCK_SIZE_M,)\n",
    "\n",
    "\n",
    "\n",
    "    acc_o = acc * 1/l_i[:, None] # Shape: (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "\n",
    "    # 3. Store Output O\n",
    "    # Output pointers: Shape (BLOCK_SIZE_M, BLOCK_SIZE_K)\n",
    "    o_ptrs = (O + batch_id * stride_oz + head_id * stride_oh +\n",
    "              offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n",
    "    # Boundary check mask for O (same as Q mask for rows, implicit for cols)\n",
    "    o_mask = offs_m[:, None] < N_CTX\n",
    "    # Store the result (convert to output dtype if necessary)\n",
    "    tl.store(o_ptrs, acc_o.to(Q.dtype.element_ty), mask=o_mask)\n",
    "\n",
    "    # 4. Store LogSumExp (LSE)\n",
    "    # LSE pointers: Shape (BLOCK_SIZE_M,)\n",
    "    lse_ptrs = (LSE + batch_id * stride_lse_z + head_id * stride_lse_h +\n",
    "                offs_m * stride_lse_m)\n",
    "    # Boundary check mask for LSE rows\n",
    "    lse_mask = offs_m < N_CTX\n",
    "    # Store LSE\n",
    "    tl.store(lse_ptrs, lse_final, mask=lse_mask)\n",
    "\n",
    "\n",
    "# --- Wrapper Function (Example) ---\n",
    "def flash_attn_v2_fwd(q, k, v, causal=False):\n",
    "    # q, k, v: (Z, H, N, D) tensors\n",
    "    Z, H, N_CTX, D_HEAD = q.shape\n",
    "    assert D_HEAD == k.shape[-1] and D_HEAD == v.shape[-1]\n",
    "    assert k.shape[0] == Z and k.shape[1] == H and k.shape[2] == N_CTX\n",
    "    assert v.shape[0] == Z and v.shape[1] == H and v.shape[2] == N_CTX\n",
    "\n",
    "    # Output tensor\n",
    "    o = torch.empty_like(q)\n",
    "    # LSE tensor: (Batch, Head, SeqLen) - stores logsumexp for backward pass\n",
    "    lse = torch.empty((Z, H, N_CTX), device=q.device, dtype=torch.float32)\n",
    "\n",
    "    # Choose block sizes (heuristic, may need tuning)\n",
    "    BLOCK_SIZE_M = 128\n",
    "    BLOCK_SIZE_N = 64\n",
    "    # BLOCK_SIZE_K must be D_HEAD in this implementation\n",
    "    BLOCK_SIZE_K = D_HEAD\n",
    "\n",
    "    # Check if D_HEAD is a power of 2 <= 128 (common constraint)\n",
    "    if D_HEAD not in [16, 32, 64, 128]:\n",
    "         print(f\"Warning: D_HEAD={D_HEAD} might not be optimal or supported by all Triton configurations.\")\n",
    "         # Adjust BLOCK_SIZE_K if needed, but this kernel assumes it's D_HEAD\n",
    "         assert BLOCK_SIZE_K == D_HEAD, \"This kernel requires BLOCK_SIZE_K == D_HEAD\"\n",
    "\n",
    "\n",
    "    num_m_blocks = triton.cdiv(N_CTX, BLOCK_SIZE_M)\n",
    "    num_bh_groups = Z * H\n",
    "\n",
    "    grid = (num_m_blocks, num_bh_groups)\n",
    "\n",
    "    # Launch kernel\n",
    "    flash_attn_v2_fwd_kernel[grid](\n",
    "        q, k, v, o, lse,\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        o.stride(0), o.stride(1), o.stride(2), o.stride(3),\n",
    "        lse.stride(0), lse.stride(1), lse.stride(2),\n",
    "        Z, H, N_CTX, D_HEAD,\n",
    "        BLOCK_SIZE_M=BLOCK_SIZE_M,\n",
    "        BLOCK_SIZE_N=BLOCK_SIZE_N,\n",
    "        BLOCK_SIZE_K=BLOCK_SIZE_K, # Pass D_HEAD as BLOCK_SIZE_K\n",
    "        IF_CAUSAL_MASK=causal,\n",
    "        #num_warps=4, # Example, tune based on block sizes and hardware\n",
    "        #num_stages=2 # Example, tune based on block sizes and hardware\n",
    "    )\n",
    "\n",
    "    return o, lse # Return output and LSE\n",
    "\n",
    "@triton.jit\n",
    "def flash_attn_v1_fwd_kernel(\n",
    "    Q, K, V, O, M, L,\n",
    "    stride_qz, stride_qh, stride_qm, stride_qk,\n",
    "    stride_kz, stride_kh, stride_kn, stride_kk,\n",
    "    stride_vz, stride_vh, stride_vn, stride_vk,\n",
    "    stride_oz, stride_oh, stride_om, stride_ok,\n",
    "    stride_mz, stride_mh, stride_mm,\n",
    "    stride_lz, stride_lh, stride_lm,\n",
    "    Z, H, N_CTX,\n",
    "    SOFTMAX_SCALE,\n",
    "    BLOCK_SIZE_M: tl.constexpr,\n",
    "    BLOCK_SIZE_N: tl.constexpr,\n",
    "    BLOCK_SIZE_HEAD_DIM: tl.constexpr,\n",
    "    IF_CAUSAL_MASK: tl.constexpr,\n",
    "):\n",
    "    start_m = tl.program_id(0)\n",
    "    batch_head_id = tl.program_id(1)\n",
    "\n",
    "    batch_id = batch_head_id // H\n",
    "    head_id = batch_head_id % H\n",
    "\n",
    "    offs_m = start_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_HEAD_DIM)\n",
    "\n",
    "    q_ptrs = (Q + batch_id * stride_qz + head_id * stride_qh +\n",
    "              offs_m[:, None] * stride_qm + offs_k[None, :] * stride_qk)\n",
    "\n",
    "    q_mask = offs_m[:, None] < N_CTX\n",
    "    q = tl.load(q_ptrs, mask=q_mask, other=0.0)\n",
    "\n",
    "    # Initialize online softmax variables\n",
    "    l_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32)\n",
    "    m_i = tl.zeros([BLOCK_SIZE_M], dtype=tl.float32) + float('-inf')\n",
    "    acc = tl.zeros([BLOCK_SIZE_M, BLOCK_SIZE_HEAD_DIM], dtype=tl.float32)\n",
    "\n",
    "    #qk_scale = tl.rsqrt(BLOCK_SIZE_HEAD_DIM)\n",
    "\n",
    "    end_n = N_CTX if not IF_CAUSAL_MASK else (start_m + 1) * BLOCK_SIZE_M\n",
    "\n",
    "    for start_n in range(0, end_n, BLOCK_SIZE_N):\n",
    "        offs_n = start_n + tl.arange(0, BLOCK_SIZE_N)\n",
    "\n",
    "        k_ptrs = (K + batch_id * stride_kz + head_id * stride_kh +\n",
    "                  offs_k[:, None] * stride_kk + offs_n[None, :] * stride_kn)\n",
    "        v_ptrs = (V + batch_id * stride_vz + head_id * stride_vh +\n",
    "                  offs_n[:, None] * stride_vn + offs_k[None, :] * stride_vk)\n",
    "\n",
    "        kv_mask = offs_n[None, :] < N_CTX # Mask applies to the dimension varying with 'n'\n",
    "        # V mask\n",
    "        v_mask = offs_n[:, None] < N_CTX\n",
    "\n",
    "        # Load K tile (shape will be BLOCK_SIZE_HEAD_DIM x BLOCK_SIZE_N due to pointer layout)\n",
    "        k = tl.load(k_ptrs, mask=kv_mask, other=0.0)\n",
    "        # Load V tile (shape will be BLOCK_SIZE_N x BLOCK_SIZE_HEAD_DIM)\n",
    "        v = tl.load(v_ptrs, mask=v_mask, other=0.0)\n",
    "\n",
    "        # Compute attention scores\n",
    "        qk = tl.dot(q, k)\n",
    "        qk *= SOFTMAX_SCALE\n",
    "\n",
    "        # Apply causal mask if needed\n",
    "        if IF_CAUSAL_MASK:\n",
    "            causal_mask = offs_m[:, None] >= offs_n[None, :]\n",
    "            qk = tl.where(causal_mask, qk, float('-inf'))\n",
    "\n",
    "        # Online softmax computation\n",
    "        m_i_new = tl.maximum(m_i, tl.max(qk, axis=1))\n",
    "        p_ij = tl.exp(qk - m_i_new[:, None])\n",
    "        scale = tl.exp(m_i - m_i_new)\n",
    "\n",
    "        # Update accumulator\n",
    "        acc = acc * scale[:, None]\n",
    "        acc += tl.dot(p_ij.to(v.dtype), v)\n",
    "\n",
    "        # Update normalizing factors\n",
    "        l_i_current = tl.sum(p_ij, axis=1)\n",
    "        l_i = l_i * scale + l_i_current\n",
    "        m_i = m_i_new\n",
    "\n",
    "    # Store outputs\n",
    "    O_ptrs = (O + batch_id * stride_oz + head_id * stride_oh +\n",
    "              offs_m[:, None] * stride_om + offs_k[None, :] * stride_ok)\n",
    "    M_ptrs = (M + batch_id * stride_mz + head_id * stride_mh + offs_m * stride_mm)\n",
    "    L_ptrs = (L + batch_id * stride_lz + head_id * stride_lh + offs_m * stride_lm)\n",
    "\n",
    "    acc_o = acc / l_i[:, None]\n",
    "\n",
    "    o_mask = offs_m[:, None] < N_CTX\n",
    "    m_mask = offs_m < N_CTX\n",
    "\n",
    "    tl.store(O_ptrs, acc_o, mask=o_mask)\n",
    "    tl.store(M_ptrs, m_i, mask=m_mask)\n",
    "    tl.store(L_ptrs, l_i, mask=m_mask)\n",
    "\n",
    "\n",
    "def flash_attention_v1_forward(q, k, v, causal=False):\n",
    "  \n",
    "    batch_size, num_heads, seq_len, head_dim = q.shape\n",
    "\n",
    "    block_m=64 #Hard coded please check the triton heuristice fucntion for the same\n",
    "    block_n=64\n",
    "\n",
    "    # Create output tensors\n",
    "    output = torch.empty_like(q)\n",
    "    m = torch.empty((batch_size, num_heads, seq_len), device=q.device, dtype=torch.float32)\n",
    "    l = torch.empty((batch_size, num_heads, seq_len), device=q.device, dtype=torch.float32)\n",
    "\n",
    "    softmax_scale=1/math.sqrt(head_dim)\n",
    "\n",
    "    # Calculate grid dimensions\n",
    "    grid_m = triton.cdiv(seq_len, block_m)\n",
    "    grid = (grid_m, batch_size * num_heads)\n",
    "\n",
    "    # Launch kernel\n",
    "    flash_attn_v1_fwd_kernel[grid](\n",
    "        q, k, v, output, m, l,\n",
    "        # Q strides\n",
    "        q.stride(0), q.stride(1), q.stride(2), q.stride(3),\n",
    "        # K strides\n",
    "        k.stride(0), k.stride(1), k.stride(2), k.stride(3),\n",
    "        # V strides\n",
    "        v.stride(0), v.stride(1), v.stride(2), v.stride(3),\n",
    "        # O strides\n",
    "        output.stride(0), output.stride(1), output.stride(2), output.stride(3),\n",
    "        # M strides\n",
    "        m.stride(0), m.stride(1), m.stride(2),\n",
    "        # L strides\n",
    "        l.stride(0), l.stride(1), l.stride(2),\n",
    "        # Dimensions\n",
    "        batch_size, num_heads, seq_len,\n",
    "        #Softmax_scale\n",
    "        SOFTMAX_SCALE=softmax_scale,\n",
    "        # Block sizes\n",
    "        BLOCK_SIZE_M=block_m,\n",
    "        BLOCK_SIZE_N=block_n,\n",
    "        BLOCK_SIZE_HEAD_DIM=head_dim,\n",
    "        IF_CAUSAL_MASK=causal,\n",
    "    )\n",
    "\n",
    "    return output, m, l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Benchmark utilities\n",
    "def get_gpu_memory():\n",
    "    \"\"\"Get current GPU memory usage in MB\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.memory_allocated() / 1024**2\n",
    "    return 0\n",
    "\n",
    "def benchmark_function(func, *args, num_warmup=5, num_runs=10):\n",
    "    \"\"\"Benchmark a function with warmup and multiple runs\"\"\"\n",
    "    # Warmup\n",
    "    for _ in range(num_warmup):\n",
    "        with torch.no_grad():\n",
    "            func(*args)\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Measure memory before\n",
    "    mem_before = get_gpu_memory()\n",
    "    \n",
    "    # Actual timing\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for _ in range(num_runs):\n",
    "        with torch.no_grad():\n",
    "            result = func(*args)\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Measure memory after\n",
    "    mem_after = get_gpu_memory()\n",
    "    memory_used = mem_after - mem_before\n",
    "    \n",
    "    avg_time = (end_time - start_time) / num_runs\n",
    "    return avg_time, memory_used, result\n",
    "\n",
    "\n",
    "\n",
    "# Attention implementations to compare\n",
    "def pytorch_attention(q, k, v, causal=False):\n",
    "    \"\"\"Standard PyTorch attention implementation\"\"\"\n",
    "    scale = 1.0 / (q.size(-1) ** 0.5)\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
    "    \n",
    "    if causal:\n",
    "        seq_len = q.size(-2)\n",
    "        mask = torch.triu(torch.ones(seq_len, seq_len, device=q.device), diagonal=1).bool()\n",
    "        scores.masked_fill_(mask, float('-inf'))\n",
    "    \n",
    "    attn_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attn_weights, v)\n",
    "    return output\n",
    "\n",
    "def pytorch_sdpa(q, k, v, causal=False):\n",
    "    \"\"\"PyTorch Scaled Dot Product Attention (optimized)\"\"\"\n",
    "    return F.scaled_dot_product_attention(q, k, v, is_causal=causal)\n",
    "\n",
    "def triton_flash_attention_v2(q, k, v, causal=False):\n",
    "    \"\"\"Our Triton Flash Attention V2 implementation\"\"\"\n",
    "    output, _ = flash_attn_v2_fwd(q, k, v, causal=causal)\n",
    "    return output\n",
    "\n",
    "def triton_flash_attention_v1(q, k, v, causal=False):\n",
    "    \"\"\"Our Triton Flash Attention V1 implementation\"\"\"\n",
    "    output, _, _ = flash_attention_v1_forward(q, k, v, causal=causal)\n",
    "    return output\n",
    "\n",
    "class AttentionBenchmark:\n",
    "    def __init__(self):\n",
    "        self.implementations = {\n",
    "            'PyTorch Standard': pytorch_attention,\n",
    "            'PyTorch SDPA': pytorch_sdpa,\n",
    "            'Triton Flash Attn V1': triton_flash_attention_v1,\n",
    "            'Triton Flash Attn V2': triton_flash_attention_v2\n",
    "        }\n",
    "        self.results = {name: {'times': [], 'memory': [], 'seq_lens': []} \n",
    "                       for name in self.implementations.keys()}\n",
    "    \n",
    "    def run_benchmark(self, seq_lens: List[int], batch_size: int = 2, \n",
    "                     num_heads: int = 8, head_dim: int = 64, \n",
    "                     causal: bool = True, dtype=torch.float16):\n",
    "        \"\"\"Run benchmark across different sequence lengths\"\"\"\n",
    "        \n",
    "        print(f\"Running benchmark with:\")\n",
    "        print(f\"  Batch size: {batch_size}\")\n",
    "        print(f\"  Num heads: {num_heads}\")\n",
    "        print(f\"  Head dim: {head_dim}\")\n",
    "        print(f\"  Causal: {causal}\")\n",
    "        print(f\"  Data type: {dtype}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for seq_len in seq_lens:\n",
    "            print(f\"\\nSequence length: {seq_len}\")\n",
    "            \n",
    "            # Create test tensors\n",
    "            q = torch.randn(batch_size, num_heads, seq_len, head_dim, \n",
    "                          dtype=dtype, device='cuda', requires_grad=False)\n",
    "            k = torch.randn(batch_size, num_heads, seq_len, head_dim, \n",
    "                          dtype=dtype, device='cuda', requires_grad=False)\n",
    "            v = torch.randn(batch_size, num_heads, seq_len, head_dim, \n",
    "                          dtype=dtype, device='cuda', requires_grad=False)\n",
    "            \n",
    "            for name, func in self.implementations.items():\n",
    "                try:\n",
    "                    # Clear cache before each test\n",
    "                    torch.cuda.empty_cache()\n",
    "                    gc.collect()\n",
    "                    \n",
    "                    print(f\"  Testing {name}...\")\n",
    "                    avg_time, memory_used, _ = benchmark_function(func, q, k, v, causal)\n",
    "                    \n",
    "                    self.results[name]['times'].append(avg_time * 1000)  # Convert to ms\n",
    "                    self.results[name]['memory'].append(memory_used)\n",
    "                    self.results[name]['seq_lens'].append(seq_len)\n",
    "                    \n",
    "                    print(f\"    Time: {avg_time*1000:.2f}ms, Memory: {memory_used:.4f}MB\")\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    Error: {str(e)}\")\n",
    "                    # Still append data points to keep arrays aligned\n",
    "                    self.results[name]['times'].append(float('nan'))\n",
    "                    self.results[name]['memory'].append(float('nan'))\n",
    "                    self.results[name]['seq_lens'].append(seq_len)\n",
    "            \n",
    "            # Clear tensors\n",
    "            del q, k, v\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Plot benchmark results\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "        \n",
    "        # Plot execution time\n",
    "        ax1.set_title('Execution Time vs Sequence Length')\n",
    "        ax1.set_xlabel('Sequence Length')\n",
    "        ax1.set_ylabel('Time (ms)')\n",
    "        ax1.set_xscale('log', base=2)\n",
    "        ax1.set_yscale('log')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        colors = ['blue', 'red', 'green', 'purple']\n",
    "        markers = ['o', 's', '^', 'D']\n",
    "        \n",
    "        for i, (name, data) in enumerate(self.results.items()):\n",
    "            # Filter out NaN values\n",
    "            valid_indices = [j for j, t in enumerate(data['times']) if not np.isnan(t)]\n",
    "            if valid_indices:\n",
    "                seq_lens = [data['seq_lens'][j] for j in valid_indices]\n",
    "                times = [data['times'][j] for j in valid_indices]\n",
    "                color = colors[i % len(colors)]\n",
    "                marker = markers[i % len(markers)]\n",
    "                ax1.plot(seq_lens, times, marker=marker, color=color, \n",
    "                        label=name, linewidth=2, markersize=6)\n",
    "        \n",
    "        ax1.legend()\n",
    "        \n",
    "        # Plot memory usage\n",
    "        ax2.set_title('Memory Usage vs Sequence Length')\n",
    "        ax2.set_xlabel('Sequence Length')\n",
    "        ax2.set_ylabel('Memory Usage (MB)')\n",
    "        ax2.set_xscale('log', base=2)\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        for i, (name, data) in enumerate(self.results.items()):\n",
    "            # Filter out NaN values\n",
    "            valid_indices = [j for j, m in enumerate(data['memory']) if not np.isnan(m)]\n",
    "            if valid_indices:\n",
    "                seq_lens = [data['seq_lens'][j] for j in valid_indices]\n",
    "                memory = [data['memory'][j] for j in valid_indices]\n",
    "                color = colors[i % len(colors)]\n",
    "                marker = markers[i % len(markers)]\n",
    "                ax2.plot(seq_lens, memory, marker=marker, color=color, \n",
    "                        label=name, linewidth=2, markersize=6)\n",
    "        \n",
    "        ax2.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('attention_benchmark.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        # Print summary table\n",
    "        self.print_summary()\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print a summary table of results\"\"\"\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"BENCHMARK SUMMARY\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Find the sequence length with all implementations working\n",
    "        common_seq_lens = []\n",
    "        for seq_len in self.results['PyTorch Standard']['seq_lens']:\n",
    "            if all(seq_len in self.results[name]['seq_lens'] for name in self.results.keys()):\n",
    "                # Check if all implementations have valid results for this seq_len\n",
    "                seq_idx = self.results['PyTorch Standard']['seq_lens'].index(seq_len)\n",
    "                if all(not np.isnan(self.results[name]['times'][seq_idx]) for name in self.results.keys()):\n",
    "                    common_seq_lens.append(seq_len)\n",
    "        \n",
    "        if common_seq_lens:\n",
    "            # Print comparison for the largest common sequence length\n",
    "            seq_len = max(common_seq_lens)\n",
    "            print(f\"\\nComparison at sequence length {seq_len}:\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            baseline_time = None\n",
    "            for name, data in self.results.items():\n",
    "                seq_idx = data['seq_lens'].index(seq_len)\n",
    "                time_ms = data['times'][seq_idx]\n",
    "                memory_mb = data['memory'][seq_idx]\n",
    "                \n",
    "                if baseline_time is None:\n",
    "                    baseline_time = time_ms\n",
    "                    speedup = 1.0\n",
    "                else:\n",
    "                    speedup = baseline_time / time_ms\n",
    "                \n",
    "                print(f\"{name:20} | {time_ms:8.2f}ms | {memory_mb:8.1f}MB | {speedup:.2f}x speedup\")\n",
    "        \n",
    "        print(\"\\nNote: Speedup is relative to the first implementation\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main benchmark execution\"\"\"\n",
    "    if not torch.cuda.is_available():\n",
    "        print(\"CUDA is not available. This benchmark requires GPU.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Flash Attention Benchmark Suite\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Test different sequence lengths (powers of 2)\n",
    "    seq_lens = [128, 256, 512, 1024, 2048]\n",
    "    \n",
    "    # For very long sequences, test separately to avoid OOM\n",
    "    extended_seq_lens = [8192, 50000]\n",
    "    \n",
    "    benchmark = AttentionBenchmark()\n",
    "    \n",
    "    try:\n",
    "        # Run main benchmark\n",
    "        benchmark.run_benchmark(seq_lens, batch_size=2, num_heads=8, head_dim=64, causal=True)\n",
    "        \n",
    "        # Try extended sequence lengths (might OOM for some implementations)\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(\"Testing extended sequence lengths (some may fail due to memory)\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        for seq_len in extended_seq_lens:\n",
    "            try:\n",
    "                benchmark.run_benchmark([seq_len], batch_size=1, num_heads=4, head_dim=64, causal=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Extended test failed at seq_len={seq_len}: {str(e)}\")\n",
    "        \n",
    "        # Plot and summarize results\n",
    "        benchmark.plot_results()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Benchmark failed: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
